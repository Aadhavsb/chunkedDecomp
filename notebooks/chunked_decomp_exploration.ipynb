{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36f6a87",
   "metadata": {},
   "source": [
    "# ChunkedDecomp Exploration and Analysis\n",
    "\n",
    "This notebook provides interactive exploration of the ChunkedDecomp compression system.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Imports\n",
    "2. Basic SVD Compression Exploration\n",
    "3. Chunked Compression Analysis\n",
    "4. Model Compression Examples\n",
    "5. Memory Usage Analysis\n",
    "6. Performance Evaluation\n",
    "7. Visualization and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0283c71",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533175b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from models.chunked_decomp import ChunkedDecomp\n",
    "from models.kv_cache import ChunkedKVCache\n",
    "from models.compressed_model import CompressedModelWrapper\n",
    "from utils.svd_utils import SVDCompressor\n",
    "from utils.memory_utils import MemoryTracker\n",
    "from utils.data_utils import DatasetManager\n",
    "from evaluation.performance_evaluator import PerformanceEvaluator\n",
    "from evaluation.memory_profiler import MemoryProfiler\n",
    "\n",
    "# Setup plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Device setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d72fd06",
   "metadata": {},
   "source": [
    "## 2. Basic SVD Compression Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d494bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SVD compressor\n",
    "compressor = SVDCompressor(device=device)\n",
    "\n",
    "# Create test matrices of different sizes\n",
    "matrices = {\n",
    "    'small': torch.randn(64, 128, device=device),\n",
    "    'medium': torch.randn(256, 512, device=device),\n",
    "    'large': torch.randn(512, 1024, device=device),\n",
    "    'square': torch.randn(512, 512, device=device)\n",
    "}\n",
    "\n",
    "print(\"Matrix shapes:\")\n",
    "for name, matrix in matrices.items():\n",
    "    print(f\"{name}: {matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore compression ratios\n",
    "compression_ratios = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "matrix_name = 'medium'\n",
    "test_matrix = matrices[matrix_name]\n",
    "\n",
    "results = []\n",
    "\n",
    "for ratio in tqdm(compression_ratios, desc=\"Testing compression ratios\"):\n",
    "    # Compress matrix\n",
    "    compressed_data = compressor.compress_matrix(\n",
    "        test_matrix,\n",
    "        compression_ratio=ratio,\n",
    "        collect_stats=True\n",
    "    )\n",
    "    \n",
    "    # Decompress\n",
    "    decompressed_matrix = compressor.decompress_matrix(compressed_data)\n",
    "    \n",
    "    # Calculate error\n",
    "    error_analysis = compressor.analyze_compression_error(test_matrix, compressed_data)\n",
    "    \n",
    "    results.append({\n",
    "        'compression_ratio': ratio,\n",
    "        'rank': compressed_data['rank'],\n",
    "        'relative_error': error_analysis['relative_error'],\n",
    "        'memory_reduction_mb': compressed_data['compression_stats']['memory_reduction_mb'],\n",
    "        'compression_time_ms': compressed_data['compression_stats']['compression_time_ms']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Compression analysis complete!\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc1d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize compression trade-offs\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(f'SVD Compression Analysis - {matrix_name} matrix ({test_matrix.shape})', fontsize=16)\n",
    "\n",
    "# Compression ratio vs relative error\n",
    "axes[0, 0].plot(results_df['compression_ratio'], results_df['relative_error'], 'o-', linewidth=2, markersize=6)\n",
    "axes[0, 0].set_xlabel('Compression Ratio')\n",
    "axes[0, 0].set_ylabel('Relative Error')\n",
    "axes[0, 0].set_title('Compression vs Accuracy Trade-off')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Compression ratio vs memory reduction\n",
    "axes[0, 1].plot(results_df['compression_ratio'], results_df['memory_reduction_mb'], 'o-', linewidth=2, markersize=6, color='green')\n",
    "axes[0, 1].set_xlabel('Compression Ratio')\n",
    "axes[0, 1].set_ylabel('Memory Reduction (MB)')\n",
    "axes[0, 1].set_title('Memory Savings')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Rank vs relative error\n",
    "axes[1, 0].plot(results_df['rank'], results_df['relative_error'], 'o-', linewidth=2, markersize=6, color='red')\n",
    "axes[1, 0].set_xlabel('Rank')\n",
    "axes[1, 0].set_ylabel('Relative Error')\n",
    "axes[1, 0].set_title('Rank vs Accuracy')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Compression time\n",
    "axes[1, 1].plot(results_df['compression_ratio'], results_df['compression_time_ms'], 'o-', linewidth=2, markersize=6, color='purple')\n",
    "axes[1, 1].set_xlabel('Compression Ratio')\n",
    "axes[1, 1].set_ylabel('Compression Time (ms)')\n",
    "axes[1, 1].set_title('Compression Speed')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57aeb41",
   "metadata": {},
   "source": [
    "## 3. Chunked Compression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158df609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare chunked vs non-chunked compression\n",
    "large_matrix = torch.randn(1024, 2048, device=device)\n",
    "chunk_sizes = [32, 64, 128, 256, 512]\n",
    "compression_ratio = 0.5\n",
    "\n",
    "chunked_results = []\n",
    "\n",
    "# Non-chunked compression\n",
    "print(\"Testing non-chunked compression...\")\n",
    "start_time = torch.cuda.Event(enable_timing=True) if device == 'cuda' else None\n",
    "end_time = torch.cuda.Event(enable_timing=True) if device == 'cuda' else None\n",
    "\n",
    "if device == 'cuda':\n",
    "    start_time.record()\n",
    "\n",
    "non_chunked_data = compressor.compress_matrix(\n",
    "    large_matrix,\n",
    "    compression_ratio=compression_ratio,\n",
    "    collect_stats=True\n",
    ")\n",
    "\n",
    "if device == 'cuda':\n",
    "    end_time.record()\n",
    "    torch.cuda.synchronize()\n",
    "    non_chunked_time = start_time.elapsed_time(end_time)\n",
    "else:\n",
    "    non_chunked_time = non_chunked_data['compression_stats']['compression_time_ms']\n",
    "\n",
    "non_chunked_decompressed = compressor.decompress_matrix(non_chunked_data)\n",
    "non_chunked_error = torch.norm(large_matrix - non_chunked_decompressed) / torch.norm(large_matrix)\n",
    "\n",
    "print(f\"Non-chunked: {non_chunked_time:.2f}ms, error: {non_chunked_error:.4f}\")\n",
    "\n",
    "# Chunked compression\n",
    "for chunk_size in tqdm(chunk_sizes, desc=\"Testing chunk sizes\"):\n",
    "    if device == 'cuda':\n",
    "        start_time.record()\n",
    "    \n",
    "    chunked_data = compressor.compress_matrix(\n",
    "        large_matrix,\n",
    "        compression_ratio=compression_ratio,\n",
    "        chunk_size=chunk_size,\n",
    "        collect_stats=True\n",
    "    )\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        end_time.record()\n",
    "        torch.cuda.synchronize()\n",
    "        chunked_time = start_time.elapsed_time(end_time)\n",
    "    else:\n",
    "        chunked_time = chunked_data['compression_stats']['compression_time_ms']\n",
    "    \n",
    "    chunked_decompressed = compressor.decompress_matrix(chunked_data)\n",
    "    chunked_error = torch.norm(large_matrix - chunked_decompressed) / torch.norm(large_matrix)\n",
    "    \n",
    "    chunked_results.append({\n",
    "        'chunk_size': chunk_size,\n",
    "        'compression_time_ms': chunked_time,\n",
    "        'relative_error': chunked_error.item(),\n",
    "        'num_chunks': chunked_data.get('num_chunks', 1)\n",
    "    })\n",
    "\n",
    "chunked_df = pd.DataFrame(chunked_results)\n",
    "print(\"\\nChunked compression results:\")\n",
    "print(chunked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize chunked vs non-chunked comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Compression time comparison\n",
    "axes[0].axhline(y=non_chunked_time, color='red', linestyle='--', label=f'Non-chunked ({non_chunked_time:.1f}ms)')\n",
    "axes[0].plot(chunked_df['chunk_size'], chunked_df['compression_time_ms'], 'o-', linewidth=2, markersize=8, label='Chunked')\n",
    "axes[0].set_xlabel('Chunk Size')\n",
    "axes[0].set_ylabel('Compression Time (ms)')\n",
    "axes[0].set_title('Compression Speed Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error comparison\n",
    "axes[1].axhline(y=non_chunked_error, color='red', linestyle='--', label=f'Non-chunked ({non_chunked_error:.4f})')\n",
    "axes[1].plot(chunked_df['chunk_size'], chunked_df['relative_error'], 'o-', linewidth=2, markersize=8, label='Chunked')\n",
    "axes[1].set_xlabel('Chunk Size')\n",
    "axes[1].set_ylabel('Relative Error')\n",
    "axes[1].set_title('Accuracy Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Number of chunks\n",
    "axes[2].plot(chunked_df['chunk_size'], chunked_df['num_chunks'], 'o-', linewidth=2, markersize=8, color='green')\n",
    "axes[2].set_xlabel('Chunk Size')\n",
    "axes[2].set_ylabel('Number of Chunks')\n",
    "axes[2].set_title('Chunking Strategy')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028db2bb",
   "metadata": {},
   "source": [
    "## 4. Model Compression Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a84408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple transformer-like model\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=1000, hidden_size=256, num_layers=4, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.pos_embedding = nn.Embedding(512, hidden_size)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_size,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=hidden_size * 4,\n",
    "                batch_first=True\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        pos_ids = torch.arange(seq_len, device=x.device).unsqueeze(0).expand_as(x)\n",
    "        \n",
    "        x = self.embedding(x) + self.pos_embedding(pos_ids)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return self.output_projection(x)\n",
    "\n",
    "# Create model\n",
    "model = SimpleTransformer().to(device)\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Test input\n",
    "test_input = torch.randint(0, 1000, (2, 32), device=device)\n",
    "with torch.no_grad():\n",
    "    original_output = model(test_input)\n",
    "    \n",
    "print(f\"Original output shape: {original_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress the model with different configurations\n",
    "compression_configs = [\n",
    "    {'compression_ratio': 0.3, 'chunk_size': 32, 'adaptive_rank': False},\n",
    "    {'compression_ratio': 0.5, 'chunk_size': 64, 'adaptive_rank': False},\n",
    "    {'compression_ratio': 0.7, 'chunk_size': 64, 'adaptive_rank': False},\n",
    "    {'compression_ratio': 0.5, 'chunk_size': 64, 'adaptive_rank': True, 'error_threshold': 0.1}\n",
    "]\n",
    "\n",
    "compression_results = []\n",
    "\n",
    "for i, config in enumerate(compression_configs):\n",
    "    print(f\"\\nTesting configuration {i+1}: {config}\")\n",
    "    \n",
    "    # Create fresh model copy\n",
    "    model_copy = SimpleTransformer().to(device)\n",
    "    model_copy.load_state_dict(model.state_dict())\n",
    "    \n",
    "    # Create compression config\n",
    "    full_config = {\n",
    "        'compression': config,\n",
    "        'kv_cache': {\n",
    "            'max_cache_size': 1000000,\n",
    "            'compression_threshold': 0.8\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Apply compression\n",
    "    with MemoryTracker(device=device) as tracker:\n",
    "        chunked_decomp = ChunkedDecomp(model=model_copy, config=full_config)\n",
    "        compression_stats = chunked_decomp.apply_compression()\n",
    "    \n",
    "    memory_stats = tracker.get_stats()\n",
    "    \n",
    "    # Test compressed model\n",
    "    model_copy.eval()\n",
    "    with torch.no_grad():\n",
    "        compressed_output = model_copy(test_input)\n",
    "    \n",
    "    # Calculate output difference\n",
    "    output_diff = torch.norm(original_output - compressed_output) / torch.norm(original_output)\n",
    "    \n",
    "    result = {\n",
    "        'config_index': i,\n",
    "        'compression_ratio': config['compression_ratio'],\n",
    "        'chunk_size': config['chunk_size'],\n",
    "        'adaptive_rank': config.get('adaptive_rank', False),\n",
    "        'layers_compressed': compression_stats['layers_compressed'],\n",
    "        'memory_reduction_mb': compression_stats['memory_reduction_mb'],\n",
    "        'compression_time_s': compression_stats['compression_time_seconds'],\n",
    "        'output_difference': output_diff.item(),\n",
    "        'peak_memory_mb': memory_stats['peak_memory_mb']\n",
    "    }\n",
    "    \n",
    "    compression_results.append(result)\n",
    "    \n",
    "    print(f\"  Layers compressed: {compression_stats['layers_compressed']}\")\n",
    "    print(f\"  Memory reduction: {compression_stats['memory_reduction_mb']:.2f} MB\")\n",
    "    print(f\"  Output difference: {output_diff:.4f}\")\n",
    "\n",
    "compression_df = pd.DataFrame(compression_results)\n",
    "print(\"\\nCompression comparison:\")\n",
    "print(compression_df[['compression_ratio', 'chunk_size', 'adaptive_rank', \n",
    "                     'memory_reduction_mb', 'output_difference', 'compression_time_s']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58019d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model compression results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Model Compression Analysis', fontsize=16)\n",
    "\n",
    "# Memory reduction vs compression ratio\n",
    "scatter1 = axes[0, 0].scatter(compression_df['compression_ratio'], compression_df['memory_reduction_mb'], \n",
    "                             c=compression_df['chunk_size'], cmap='viridis', s=100, alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Compression Ratio')\n",
    "axes[0, 0].set_ylabel('Memory Reduction (MB)')\n",
    "axes[0, 0].set_title('Memory Savings vs Compression Ratio')\n",
    "plt.colorbar(scatter1, ax=axes[0, 0], label='Chunk Size')\n",
    "\n",
    "# Output difference vs compression ratio\n",
    "colors = ['red' if adaptive else 'blue' for adaptive in compression_df['adaptive_rank']]\n",
    "axes[0, 1].scatter(compression_df['compression_ratio'], compression_df['output_difference'], \n",
    "                  c=colors, s=100, alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Compression Ratio')\n",
    "axes[0, 1].set_ylabel('Output Difference')\n",
    "axes[0, 1].set_title('Accuracy vs Compression Ratio')\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='blue', label='Fixed Rank'),\n",
    "                  Patch(facecolor='red', label='Adaptive Rank')]\n",
    "axes[0, 1].legend(handles=legend_elements)\n",
    "\n",
    "# Compression time vs memory reduction\n",
    "axes[1, 0].scatter(compression_df['memory_reduction_mb'], compression_df['compression_time_s'], \n",
    "                  s=100, alpha=0.7, color='green')\n",
    "axes[1, 0].set_xlabel('Memory Reduction (MB)')\n",
    "axes[1, 0].set_ylabel('Compression Time (s)')\n",
    "axes[1, 0].set_title('Compression Speed vs Memory Savings')\n",
    "\n",
    "# Trade-off: memory reduction vs output difference\n",
    "axes[1, 1].scatter(compression_df['memory_reduction_mb'], compression_df['output_difference'], \n",
    "                  c=compression_df['compression_ratio'], cmap='plasma', s=100, alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Memory Reduction (MB)')\n",
    "axes[1, 1].set_ylabel('Output Difference')\n",
    "axes[1, 1].set_title('Accuracy vs Memory Trade-off')\n",
    "plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1], label='Compression Ratio')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519cf957",
   "metadata": {},
   "source": [
    "## 5. Memory Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f482d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze memory usage with different batch sizes and sequence lengths\n",
    "if device == 'cuda':\n",
    "    batch_sizes = [1, 2, 4, 8]\n",
    "    sequence_lengths = [32, 64, 128, 256]\n",
    "    \n",
    "    memory_analysis = []\n",
    "    \n",
    "    # Test original model\n",
    "    original_model = SimpleTransformer().to(device)\n",
    "    \n",
    "    # Test compressed model (use best config from above)\n",
    "    best_config_idx = compression_df['memory_reduction_mb'].idxmax()\n",
    "    best_config = compression_configs[best_config_idx]\n",
    "    \n",
    "    compressed_model = SimpleTransformer().to(device)\n",
    "    compressed_model.load_state_dict(original_model.state_dict())\n",
    "    \n",
    "    full_config = {\n",
    "        'compression': best_config,\n",
    "        'kv_cache': {'max_cache_size': 1000000, 'compression_threshold': 0.8}\n",
    "    }\n",
    "    \n",
    "    chunked_decomp = ChunkedDecomp(model=compressed_model, config=full_config)\n",
    "    chunked_decomp.apply_compression()\n",
    "    \n",
    "    print(f\"Using best compression config: {best_config}\")\n",
    "    \n",
    "    for batch_size in tqdm(batch_sizes, desc=\"Batch sizes\"):\n",
    "        for seq_len in sequence_lengths:\n",
    "            test_input = torch.randint(0, 1000, (batch_size, seq_len), device=device)\n",
    "            \n",
    "            # Test original model\n",
    "            torch.cuda.empty_cache()\n",
    "            with MemoryTracker(device=device) as tracker:\n",
    "                original_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    _ = original_model(test_input)\n",
    "            original_memory = tracker.get_stats()['peak_memory_mb']\n",
    "            \n",
    "            # Test compressed model\n",
    "            torch.cuda.empty_cache()\n",
    "            with MemoryTracker(device=device) as tracker:\n",
    "                compressed_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    _ = compressed_model(test_input)\n",
    "            compressed_memory = tracker.get_stats()['peak_memory_mb']\n",
    "            \n",
    "            memory_reduction = (original_memory - compressed_memory) / original_memory * 100\n",
    "            \n",
    "            memory_analysis.append({\n",
    "                'batch_size': batch_size,\n",
    "                'sequence_length': seq_len,\n",
    "                'original_memory_mb': original_memory,\n",
    "                'compressed_memory_mb': compressed_memory,\n",
    "                'memory_reduction_percent': memory_reduction\n",
    "            })\n",
    "    \n",
    "    memory_df = pd.DataFrame(memory_analysis)\n",
    "    print(\"\\nMemory analysis complete!\")\n",
    "    print(memory_df.head(10))\n",
    "else:\n",
    "    print(\"Memory analysis skipped (requires CUDA)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c9c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize memory usage analysis (if CUDA available)\n",
    "if device == 'cuda' and 'memory_df' in locals():\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Memory usage heatmaps\n",
    "    pivot_original = memory_df.pivot(index='sequence_length', columns='batch_size', values='original_memory_mb')\n",
    "    pivot_compressed = memory_df.pivot(index='sequence_length', columns='batch_size', values='compressed_memory_mb')\n",
    "    pivot_reduction = memory_df.pivot(index='sequence_length', columns='batch_size', values='memory_reduction_percent')\n",
    "    \n",
    "    # Original memory usage\n",
    "    sns.heatmap(pivot_original, annot=True, fmt='.1f', cmap='Reds', ax=axes[0])\n",
    "    axes[0].set_title('Original Model Memory (MB)')\n",
    "    \n",
    "    # Compressed memory usage\n",
    "    sns.heatmap(pivot_compressed, annot=True, fmt='.1f', cmap='Blues', ax=axes[1])\n",
    "    axes[1].set_title('Compressed Model Memory (MB)')\n",
    "    \n",
    "    # Memory reduction percentage\n",
    "    sns.heatmap(pivot_reduction, annot=True, fmt='.1f', cmap='Greens', ax=axes[2])\n",
    "    axes[2].set_title('Memory Reduction (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nMemory Analysis Summary:\")\n",
    "    print(f\"Average memory reduction: {memory_df['memory_reduction_percent'].mean():.1f}%\")\n",
    "    print(f\"Best memory reduction: {memory_df['memory_reduction_percent'].max():.1f}%\")\n",
    "    print(f\"Worst memory reduction: {memory_df['memory_reduction_percent'].min():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880b2392",
   "metadata": {},
   "source": [
    "## 6. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17865ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance evaluator\n",
    "evaluator = PerformanceEvaluator(device=device)\n",
    "\n",
    "# Create test data\n",
    "data_manager = DatasetManager()\n",
    "\n",
    "# Generate synthetic data for testing\n",
    "test_data = []\n",
    "for _ in range(100):  # 100 samples\n",
    "    seq_len = torch.randint(10, 64, (1,)).item()\n",
    "    tokens = torch.randint(0, 1000, (seq_len,))\n",
    "    test_data.append({'input_ids': tokens})\n",
    "\n",
    "# Create dataloader\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Pad sequences to same length\n",
    "    max_len = max(len(item['input_ids']) for item in batch)\n",
    "    \n",
    "    padded_batch = []\n",
    "    for item in batch:\n",
    "        input_ids = item['input_ids']\n",
    "        padded = torch.cat([input_ids, torch.zeros(max_len - len(input_ids), dtype=torch.long)])\n",
    "        padded_batch.append({'input_ids': padded})\n",
    "    \n",
    "    # Stack into batch tensor\n",
    "    batch_input_ids = torch.stack([item['input_ids'] for item in padded_batch])\n",
    "    return {'input_ids': batch_input_ids}\n",
    "\n",
    "test_dataset = SimpleDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Created test dataset with {len(test_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75379a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate original vs compressed model performance\n",
    "original_model = SimpleTransformer().to(device)\n",
    "\n",
    "# Create compressed model\n",
    "compressed_model = SimpleTransformer().to(device)\n",
    "compressed_model.load_state_dict(original_model.state_dict())\n",
    "\n",
    "best_config = {\n",
    "    'compression': {'compression_ratio': 0.5, 'chunk_size': 64, 'adaptive_rank': True},\n",
    "    'kv_cache': {'max_cache_size': 1000000, 'compression_threshold': 0.8}\n",
    "}\n",
    "\n",
    "chunked_decomp = ChunkedDecomp(model=compressed_model, config=best_config)\n",
    "chunked_decomp.apply_compression()\n",
    "\n",
    "print(\"Evaluating original model...\")\n",
    "original_results = evaluator.evaluate_model(\n",
    "    model=original_model,\n",
    "    test_loader=test_loader,\n",
    "    max_samples=50\n",
    ")\n",
    "\n",
    "print(\"Evaluating compressed model...\")\n",
    "compressed_results = evaluator.evaluate_model(\n",
    "    model=compressed_model,\n",
    "    test_loader=test_loader,\n",
    "    max_samples=50\n",
    ")\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(f\"Original - Avg Inference Time: {original_results.get('avg_inference_time_ms', 0):.2f}ms\")\n",
    "print(f\"Compressed - Avg Inference Time: {compressed_results.get('avg_inference_time_ms', 0):.2f}ms\")\n",
    "\n",
    "if 'loss' in original_results and 'loss' in compressed_results:\n",
    "    print(f\"Original - Loss: {original_results['loss']:.4f}\")\n",
    "    print(f\"Compressed - Loss: {compressed_results['loss']:.4f}\")\n",
    "    print(f\"Loss difference: {abs(original_results['loss'] - compressed_results['loss']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ecb71",
   "metadata": {},
   "source": [
    "## 7. Visualization and Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9cc783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary visualization\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# SVD compression trade-off\n",
    "ax1 = fig.add_subplot(gs[0, 0:2])\n",
    "ax1.plot(results_df['compression_ratio'], results_df['relative_error'], 'o-', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Compression Ratio')\n",
    "ax1.set_ylabel('Relative Error')\n",
    "ax1.set_title('SVD Compression Trade-off')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Model compression comparison\n",
    "ax2 = fig.add_subplot(gs[0, 2:4])\n",
    "bars = ax2.bar(range(len(compression_df)), compression_df['memory_reduction_mb'], alpha=0.7)\n",
    "ax2.set_xlabel('Configuration')\n",
    "ax2.set_ylabel('Memory Reduction (MB)')\n",
    "ax2.set_title('Model Compression Results')\n",
    "ax2.set_xticks(range(len(compression_df)))\n",
    "ax2.set_xticklabels([f'Config {i+1}' for i in range(len(compression_df))])\n",
    "\n",
    "# Chunked vs non-chunked\n",
    "ax3 = fig.add_subplot(gs[1, 0:2])\n",
    "if 'chunked_df' in locals():\n",
    "    ax3.plot(chunked_df['chunk_size'], chunked_df['compression_time_ms'], 'o-', linewidth=2, markersize=6, label='Chunked')\n",
    "    ax3.axhline(y=non_chunked_time, color='red', linestyle='--', label='Non-chunked')\n",
    "    ax3.set_xlabel('Chunk Size')\n",
    "    ax3.set_ylabel('Compression Time (ms)')\n",
    "    ax3.set_title('Chunked vs Non-chunked Performance')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Memory usage (if available)\n",
    "ax4 = fig.add_subplot(gs[1, 2:4])\n",
    "if device == 'cuda' and 'memory_df' in locals():\n",
    "    memory_summary = memory_df.groupby('batch_size')['memory_reduction_percent'].mean()\n",
    "    ax4.bar(memory_summary.index, memory_summary.values, alpha=0.7, color='green')\n",
    "    ax4.set_xlabel('Batch Size')\n",
    "    ax4.set_ylabel('Avg Memory Reduction (%)')\n",
    "    ax4.set_title('Memory Reduction by Batch Size')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Memory analysis\\nnot available\\n(requires CUDA)', \n",
    "             transform=ax4.transAxes, ha='center', va='center', fontsize=12)\n",
    "    ax4.set_title('Memory Analysis')\n",
    "\n",
    "# Performance comparison\n",
    "ax5 = fig.add_subplot(gs[2, 0:2])\n",
    "if 'original_results' in locals() and 'compressed_results' in locals():\n",
    "    models = ['Original', 'Compressed']\n",
    "    inference_times = [\n",
    "        original_results.get('avg_inference_time_ms', 0),\n",
    "        compressed_results.get('avg_inference_time_ms', 0)\n",
    "    ]\n",
    "    \n",
    "    bars = ax5.bar(models, inference_times, alpha=0.7, color=['blue', 'orange'])\n",
    "    ax5.set_ylabel('Avg Inference Time (ms)')\n",
    "    ax5.set_title('Inference Speed Comparison')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, time in zip(bars, inference_times):\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                f'{time:.2f}ms', ha='center', va='bottom')\n",
    "\n",
    "# Summary statistics\n",
    "ax6 = fig.add_subplot(gs[2, 2:4])\n",
    "ax6.axis('off')\n",
    "\n",
    "# Create summary text\n",
    "summary_text = \"CHUNKEDDECOMP ANALYSIS SUMMARY\\n\\n\"\n",
    "summary_text += f\"• Matrix compression tested on {len(matrices)} different sizes\\n\"\n",
    "summary_text += f\"• Best compression ratio tested: {results_df['compression_ratio'].max():.1f}\\n\"\n",
    "summary_text += f\"• Lowest relative error achieved: {results_df['relative_error'].min():.4f}\\n\\n\"\n",
    "\n",
    "if 'compression_df' in locals():\n",
    "    summary_text += f\"• Model compression configurations tested: {len(compression_df)}\\n\"\n",
    "    summary_text += f\"• Best memory reduction: {compression_df['memory_reduction_mb'].max():.2f} MB\\n\"\n",
    "    summary_text += f\"• Best output preservation: {compression_df['output_difference'].min():.4f}\\n\\n\"\n",
    "\n",
    "if device == 'cuda' and 'memory_df' in locals():\n",
    "    summary_text += f\"• Memory analysis: {len(memory_df)} configurations tested\\n\"\n",
    "    summary_text += f\"• Average memory reduction: {memory_df['memory_reduction_percent'].mean():.1f}%\\n\\n\"\n",
    "\n",
    "summary_text += f\"• Device used: {device.upper()}\\n\"\n",
    "summary_text += f\"• Chunked compression: {'✓' if 'chunked_df' in locals() else '✗'}\\n\"\n",
    "summary_text += f\"• Performance evaluation: {'✓' if 'original_results' in locals() else '✗'}\"\n",
    "\n",
    "ax6.text(0.05, 0.95, summary_text, transform=ax6.transAxes, \n",
    "         fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "\n",
    "plt.suptitle('ChunkedDecomp Comprehensive Analysis', fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb17770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final recommendations based on analysis\n",
    "print(\"=\"*60)\n",
    "print(\"CHUNKEDDECOMP RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# SVD recommendations\n",
    "best_ratio_idx = results_df['relative_error'].idxmin()\n",
    "best_ratio = results_df.loc[best_ratio_idx, 'compression_ratio']\n",
    "best_error = results_df.loc[best_ratio_idx, 'relative_error']\n",
    "\n",
    "print(f\"\\n1. SVD COMPRESSION:\")\n",
    "print(f\"   • Best compression ratio for accuracy: {best_ratio:.1f} (error: {best_error:.4f})\")\n",
    "print(f\"   • Memory-accuracy trade-off is most favorable around 0.5-0.7 range\")\n",
    "\n",
    "# Model compression recommendations\n",
    "if 'compression_df' in locals():\n",
    "    best_model_idx = compression_df['memory_reduction_mb'].idxmax()\n",
    "    best_model_config = compression_configs[best_model_idx]\n",
    "    \n",
    "    print(f\"\\n2. MODEL COMPRESSION:\")\n",
    "    print(f\"   • Best configuration: {best_model_config}\")\n",
    "    print(f\"   • Memory reduction: {compression_df.loc[best_model_idx, 'memory_reduction_mb']:.2f} MB\")\n",
    "    print(f\"   • Output difference: {compression_df.loc[best_model_idx, 'output_difference']:.4f}\")\n",
    "    print(f\"   • Adaptive rank helps maintain accuracy with aggressive compression\")\n",
    "\n",
    "# Chunking recommendations\n",
    "if 'chunked_df' in locals():\n",
    "    optimal_chunk_idx = chunked_df['compression_time_ms'].idxmin()\n",
    "    optimal_chunk_size = chunked_df.loc[optimal_chunk_idx, 'chunk_size']\n",
    "    \n",
    "    print(f\"\\n3. CHUNKING STRATEGY:\")\n",
    "    print(f\"   • Optimal chunk size for speed: {optimal_chunk_size}\")\n",
    "    print(f\"   • Chunking provides {((non_chunked_time - chunked_df['compression_time_ms'].min()) / non_chunked_time * 100):.1f}% speedup\")\n",
    "    print(f\"   • Trade-off: slight accuracy loss but significant speed improvement\")\n",
    "\n",
    "# Memory recommendations\n",
    "if device == 'cuda' and 'memory_df' in locals():\n",
    "    avg_reduction = memory_df['memory_reduction_percent'].mean()\n",
    "    print(f\"\\n4. MEMORY EFFICIENCY:\")\n",
    "    print(f\"   • Average memory reduction: {avg_reduction:.1f}%\")\n",
    "    print(f\"   • Memory savings scale well with batch size and sequence length\")\n",
    "    print(f\"   • Particularly effective for large models and long sequences\")\n",
    "\n",
    "# General recommendations\n",
    "print(f\"\\n5. GENERAL RECOMMENDATIONS:\")\n",
    "print(f\"   • Start with compression_ratio=0.5, chunk_size=64\")\n",
    "print(f\"   • Enable adaptive_rank for better accuracy preservation\")\n",
    "print(f\"   • Use chunking for large matrices (>1024x1024)\")\n",
    "print(f\"   • Monitor output quality vs memory trade-offs for your use case\")\n",
    "print(f\"   • Consider enabling KV cache for generation tasks\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
