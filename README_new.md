# ğŸ§  ChunkedDecomp: Efficient KV Cache Compression for Transformers

**ChunkedDecomp** compresses the Key-Value (KV) cache in transformer models using chunked SVD decomposition to dramatically reduce memory usage during inference while maintaining model quality.

## ğŸš€ What It Does

- ğŸ“¦ **Compresses KV cache** using low-rank chunked SVD decomposition
- ğŸ’¾ **Reduces memory usage** by 30-70% with minimal quality loss
- ğŸ” **Reconstructs on-demand** only needed chunks during inference
- ğŸ“Š **Benchmarks performance** with comprehensive evaluation metrics
- ğŸ§ª **Evaluates quality** using perplexity, BLEU, ROUGE scores
- ğŸ’» **Works everywhere** - local machines or HPC clusters (SLURM-ready)

---

## âš¡ Quick Start (3 Steps)

### Step 1: Install Dependencies
```bash
cd c:\Users\bhara\repos\chunkedDecomp
pip install -r requirements.txt
pip install -e .
```

### Step 2: Test Installation
```bash
python quick_start.py
```

### Step 3: Run Your First Compression
```bash
python scripts/run_compression.py --model_name distilgpt2 --compression_ratio 0.5
```

**That's it!** You should see compression results and memory savings.

---

## ğŸ“ Project Structure

```
chunked-decomp/
â”œâ”€â”€ configs/                    # YAML configs for compression + models
â”œâ”€â”€ data/                       # Place for input datasets
â”œâ”€â”€ notebooks/                  # Jupyter notebooks for interactive analysis
â”œâ”€â”€ results/                    # Outputs from all scripts
â”œâ”€â”€ scripts/                    # CLI scripts
â”‚   â””â”€â”€ cluster_scripts/        # SLURM job + environment setup
â”œâ”€â”€ src/                        # Core code
â”‚   â”œâ”€â”€ models/                 # ChunkedDecomp logic, KV cache handler
â”‚   â”œâ”€â”€ utils/                  # SVD, memory, dataset helpers
â”‚   â””â”€â”€ evaluation/             # Performance evaluation & memory tracking
â”œâ”€â”€ tests/                      # Unit tests
â”œâ”€â”€ requirements.txt            # Pip dependencies
â”œâ”€â”€ environment.yml             # Conda environment
â”œâ”€â”€ setup.py                    # Package installation
â””â”€â”€ README.md                   # This file
```

---

## ğŸ”¨ Usage Examples

### ğŸ”¹ Basic Compression
```bash
python scripts/run_compression.py \
  --model_name gpt2 \
  --compression_ratio 0.5 \
  --chunk_size 64 \
  --output_dir results/compression/
```

### ğŸ”¹ Evaluate Model Quality
```bash
python scripts/evaluate_model.py \
  --model_name gpt2 \
  --compression_ratio 0.5 \
  --metrics perplexity bleu \
  --dataset_name wikitext \
  --output_dir results/evaluation/
```

### ğŸ”¹ Memory Benchmarking
```bash
python scripts/benchmark_memory.py \
  --model_name gpt2 \
  --sequence_lengths 256 512 1024 \
  --batch_sizes 1 4 \
  --compression_ratios 0.3 0.5 0.7 \
  --output_dir results/memory/
```

### ğŸ”¹ Interactive Analysis
```bash
jupyter notebook
# Open notebooks/chunked_decomp_exploration.ipynb
```

---

## âš™ï¸ Installation Options

### âœ… Option 1: Conda (Recommended)
```bash
conda env create -f environment.yml
conda activate chunked-decomp
```

### âœ… Option 2: pip + venv
```bash
python -m venv venv
venv\Scripts\activate  # On Windows
pip install -r requirements.txt
pip install -e .
```

### âœ… Quick Test
```bash
python scripts/run_compression.py \
  --model_name distilgpt2 \
  --compression_ratio 0.5 \
  --num_samples 100 \
  --test_mode \
  --output_dir results/test/
```

You should see:
- Progress logs in terminal
- Files created in `results/test/`

---

## ğŸ§ª Testing

### Run Unit Tests
```bash
python -m pytest tests/ -v
```

### Run Specific Tests
```bash
python -m pytest tests/test_chunked_decomp.py -v
python -m pytest tests/test_compression.py -v
python -m pytest tests/test_kv_cache.py -v
```

---

## ğŸ› ï¸ Configuration

Use predefined configs from `configs/compression_configs.yaml`:

```python
import yaml
from src import ChunkedDecomp

with open("configs/compression_configs.yaml") as f:
    cfg = yaml.safe_load(f)

# Use balanced compression settings
compressor = ChunkedDecomp(**cfg["balanced"])
```

Available configurations:
- **Conservative**: High quality, moderate compression
- **Balanced**: Good trade-off between quality and compression
- **Aggressive**: Maximum compression, some quality loss

---

## ğŸ’» HPC Usage (SLURM)

### 1. Setup on Cluster
```bash
ssh username@cluster.edu
git clone <your-repo-url>
cd chunked-decomp
chmod +x scripts/cluster_scripts/setup_environment.sh
./scripts/cluster_scripts/setup_environment.sh
```

### 2. Submit SLURM Job
```bash
sbatch scripts/cluster_scripts/submit_job.sh
```

### 3. Monitor Job
```bash
squeue -u yourusername
```

---

## ğŸ§¾ Sample Data

Create `data/sample_input.txt`:
```
The quick brown fox jumps over the lazy dog.
Hello! This is a test input for model compression.
```

Use in scripts:
```python
with open("data/sample_input.txt") as f:
    lines = f.readlines()
```

---

## ğŸ§  Troubleshooting

| Problem | Solution |
|---------|----------|
| CUDA out of memory | Reduce batch size or sequence length |
| Low compression quality | Use `compression_ratio=0.7+` or increase `min_rank_ratio` |
| Slow reconstruction | Increase `chunk_size`, enable `use_fast_svd` |
| Import errors | Run `pip install -e .` in root folder |
| Module not found | Make sure you're in the project directory |

---

## ğŸ“Š What You Get

- **Compression statistics**: Memory reduction, parameter counts
- **Quality metrics**: Perplexity, BLEU, ROUGE scores  
- **Performance plots**: Memory vs accuracy trade-offs
- **Benchmarking results**: Speed and memory usage analysis
- **Interactive notebooks**: For experimentation and visualization

---

## ğŸ¯ Supported Models

Works with any Hugging Face transformer:
- **Small**: `distilgpt2`, `gpt2`, `distilbert-base-uncased`
- **Medium**: `gpt2-medium`, `microsoft/DialoGPT-medium`
- **Large**: `gpt2-large`, `gpt2-xl` (requires more memory)

---

## ğŸ›£ï¸ Roadmap

- [ ] Add quantization support
- [ ] LoRA compatibility
- [ ] Online adaptation for streaming models
- [ ] Multi-GPU support

---

## ğŸ‰ Summary

âœ… **Compress large transformer KV caches**  
âœ… **Run and benchmark locally or on HPC**  
âœ… **Evaluate memory savings and accuracy**  
âœ… **All with clear configs, logs, and outputs**

**Happy compressing!** ğŸš€
